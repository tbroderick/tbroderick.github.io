<html>

<head>
<meta http-equiv="content-type" content="text/html; charset=iso-8859-1"/>
<meta name="description" content="Nonparametric Bayes Tutorial"/>
<meta name="keywords" content="Nonparametric,Bayes,Tutorial"/>
<meta name="author" content="Tamara Broderick"/>
<title>Nonparametric Bayes Tutorial</title>
</head>

<body class="course-page">
<h1>Nonparametric Bayesian Methods: Models, Algorithms, and Applications</h1>

<img src="img/princeton2017bnp.jpg" align="right" height=150>
This tutorial took place at the <a href="https://bnpworkshop.github.io/">2017 Princeton Bayesian Nonparametrics Workshop</a> at Princeton University. See <a href="http://www.tamarabroderick.com/tutorials.html">this link</a> for the latest versions and videos of this tutorial.

<p>Part I: Tuesday, May 30, 10:30 AM&ndash;12:00 PM (Friend Center 006) 
<br>Part II: Wednesday, May 31, 10:30 AM&ndash;12:00 PM (Friend Center 006) 
<br>Part III: Thursday, June 1, 10:30 AM&ndash;12:00 PM (Friend Center 006) 
</p>

<p>
<b>Instructor</b>:
<br>&nbsp; Professor <a href="http://www.tamarabroderick.com">Tamara Broderick</a>
<br>&nbsp; Email: <img src="img/broderick_email.png" height=16em style="position: relative; top: 1.5px;">
</p>

<hr>

<h2>Description</h2>

Nonparametric Bayesian methods are used for data analysis in a wide variety of disciplines. These methods make use of infinite-dimensional mathematical structures to allow the practitioner to learn more from their data as the size of their data set grows. What does that mean, and how does it work in practice? In this tutorial, we'll cover why machine learning and statistics need more than just parametric Bayesian inference. We'll introduce such foundational nonparametric Bayesian models as the Dirichlet process and Chinese restaurant process and touch on the wide variety of models available in nonparametric Bayes. Along the way, we'll see what exactly nonparametric Bayesian methods are and what they accomplish.

<h2>Materials</h2>
<ul>
<li><a href="https://github.com/tbroderick/bnp_tutorial/blob/2017pton/README.txt">README</a> for demos </li>
</li>
<li><a href="files/broderick_tutorial_2017_princeton_part_i.pdf">[Slides for Part I]</a>
	<ul>
	<li>Demo 1 <a href="https://github.com/tbroderick/bnp_tutorial/blob/2017pton/ex1_beta.R">[code]</a>: Beta random variable and random distribution intuition </li>
	<li>Demo 2 <a href="https://github.com/tbroderick/bnp_tutorial/blob/2017pton/ex2_diri.R">[code]</a>: Dirichlet random variable and random distribution intuition </li>
	<li>Demo 3 <a href="https://github.com/tbroderick/bnp_tutorial/blob/2017pton/ex3_largeK_distr.R">[code]</a>: K large relative to N intuition; empty components </li>
	<li>Demo 4 <a href="https://github.com/tbroderick/bnp_tutorial/blob/2017pton/ex4_largeK_count.R">[code]</a>: K large relative to N intuition; growth of number of clusters </li>
	<li>Demo 5 <a href="https://github.com/tbroderick/bnp_tutorial/blob/2017pton/ex5_gem.R">[code]</a>: GEM random distribution intuition </li>
	</ul>
</li>
<li><a href="files/broderick_tutorial_2017_princeton_part_ii.pdf">[Slides for Part II]</a>
	<ul>
	<li>Demo 6 <a href="https://github.com/tbroderick/bnp_tutorial/blob/2017pton/ex6_dpmm.R">[code]</a>: An exact DPMM simulator </li>
	</ul>
</li>
<li><a href="files/broderick_tutorial_2017_princeton_part_iii.pdf">[Slides for Part III]</a>
	<ul>
	<li>Demo 7 <a href="https://github.com/tbroderick/bnp_tutorial/blob/2017pton/ex7_sampler.R">[code]</a>: A CRP mixture model sampler </li>
	</ul>
</li>
</ul>

<h2>Prerequisites</h2>
<ul>
<li>Know what a prior, likelihood, and posterior are.
<li>Know how to use Bayes' Theorem to calculate a posterior for both discrete and continuous parametric distributions.
<li>Understand what a generative model is.
<li>Have a basic idea of what Gibbs sampling is and when it is useful (at least check out the Wikipedia article in advance).
</ul>

<h3>What we won't cover</h3>
Gaussian processes are an important branch of nonparametric Bayesian modeling, but we won't have time to cover them here. We'll be focusing on the discrete, or Poisson point process, side of nonparametric Bayesian inference. 

</body>

</html>
