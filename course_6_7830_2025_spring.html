<html>

<head>
<meta http-equiv="content-type" content="text/html; charset=iso-8859-1"/>
<meta name="description" content="6.7830 Bayesian Modeling and Inference"/>
<meta name="keywords" content="6.7830"/>
<meta name="author" content="Tamara Broderick"/>
<title>6.7830 Bayesian Modeling and Inference</title>
</head>

<body class="course-page">
<h1>6.7830 Bayesian Modeling and Inference</h1>

<p>
Spring 2025
<br>Room <a href="http://whereis.mit.edu/?mapterms=32">32-155</a>
<br>Times: Tuesday, Thursday 2:30&ndash;4:00 PM
<br> First class: Tuesday, February 4
</p>

<p>
<b>Instructor</b>:
<br>&nbsp; Professor <a href="http://www.tamarabroderick.com">Tamara Broderick</a>
<br>&nbsp; Contact information and office hours for all staff are on <a href="https://piazza.com/mit/spring2025/67830">Piazza</a>.
</p>

<b>TAs</b>:
<br>&nbsp; <a href="https://renatoberlinghieri.github.io/">Renato Berlinghieri</a>
<br>&nbsp; <a href="https://www.mit.edu/~vishwaks/">Vishwak Srinivasan</a>

<hr>

<h2>Introduction</h2>

As both the number and size of data sets grow, practitioners are interested in learning increasingly complex information and interactions from data. Probabilistic modeling in general, and Bayesian approaches in particular, provide a unifying framework for flexible modeling that includes prediction, estimation, and coherent uncertainty quantification. In this course, we will cover modern challenges of Bayesian inference, including (but not limited to) model construction, handling large or complex data sets, and the speed and quality of approximate inference. 

<h2>Logistics</h2>

Our course Piazza page is here: <a href="https://piazza.com/mit/spring2025/67830">https://piazza.com/mit/spring2025/67830</a>
<br>All communication about the course will be through Piazza, so make sure to sign up there as soon as possible.

<p>Note that this class is heavily based on discussion, reading research papers, and active student participation.
<br>Nothing will be formally due or graded during the first week of class.

<h2>Description</h2>

This course will cover Bayesian modeling and inference at an advanced graduate level. A tentative list of topics (which may change depending on our interests) is as follows:
<ul>
<li> Introduction to Bayesian inference; motivations from de Finetti, decision theory, etc.
<li> Hierarchical modeling, including popular models such as latent Dirichlet allocation
<li> Approximate posterior inference
<li> Variational inference, mean-field, stochastic variational inference, challenges/limitations of VI, etc.
<li> Monte Carlo, avoiding random-walk behavior, Hamiltonian Monte Carlo/NUTS/Stan, etc.
<li> Evaluation, sensitivity, robustness
<li> Bayesian nonparametrics: why and how
<li> Mixture models, admixtures, Dirichlet process, Chinese restaurant process
<li> Learning functions, Gaussian processes
<li> Probabilistic numerics
<li> Bayesian optimization
</ul>

<h2>Prerequisites</h2>

<p>Requirements:  A pre-existing graduate-level familiarity with machine learning/statistics and probability is required. E.g. at MIT, 6.7800 or 6.7810 or [6.7900 and 6.7700]. (In the <a href="https://eecsis.mit.edu/numbering.html">old numbering scheme</a>, these courses would be: 6.437 or 6.438 or [6.867 and 6.436].) Past students have found Algorithms for Inference, 6.7810 (used to be 6.438), particularly useful as a prerequisite.

We will assume familiarity with graphical models, exponential families, finite-dimensional Gaussian mixture models, expectation maximization, linear & logistic regression, hidden Markov models. You can find a "problem set 0" on the Piazza page to help you gauge your background&#59; it is not graded, but you should be very comfortable solving the questions in it strictly before taking this course.


</body>

</html>
