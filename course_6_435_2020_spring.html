<html>

<head>
<meta http-equiv="content-type" content="text/html; charset=iso-8859-1"/>
<meta name="description" content="6.435 Bayesian Modeling and Inference"/>
<meta name="keywords" content="6.435"/>
<meta name="author" content="Tamara Broderick"/>
<title>6.435 Bayesian Modeling and Inference</title>
</head>

<body class="course-page">
<h1>6.435 Bayesian Modeling and Inference</h1>
Note: in 2018 and earlier, this course ran using the (temporary) number 6.882; since 2019, it has been 6.435.

<p>
Spring 2020
<br>Room <a href="http://whereis.mit.edu/?mapterms=1">1-390</a>
<br>Tuesday, Thursday 2:30&ndash;4:00 PM
<br> First class: Tuesday, February 4
</p>

<p>
<b>Instructor</b>:
<br>&nbsp; Professor <a href="http://www.tamarabroderick.com">Tamara Broderick</a>
<br>&nbsp; Office Hours: Thursday, 4&ndash;5pm, 32-G498
<br>&nbsp; Email: <img src="img/6435_2020_email.png" height=18em style="position: relative; top: 4.5px;">
</p>

<b>TAs</b>:
<br>&nbsp; Raj Agrawal, Lorenzo Masoero
<br>&nbsp; Office Hours: Tuesday, 4&ndash;5pm, 32-G475 (the G4 common area, just outside 32-G451)
<br>&nbsp; Email: <img src="img/6435_2020_email.png" height=18em style="position: relative; top: 4.5px;">
</p>

<hr>

<h2>Introduction</h2>

As both the number and size of data sets grow, practitioners are interested in learning increasingly complex information and interactions from data. Probabilistic modeling in general, and Bayesian approaches in particular, provide a unifying framework for flexible modeling that includes prediction, estimation, and coherent uncertainty quantification. In this course, we will cover the modern challenges of Bayesian inference, including (but not limited to) speed of approximate inference, making use of distributed architectures, streaming data, and complex data interactions. We will study Bayesian nonparametric models, wherein model complexity grows with the size of the data; this allows us to learn, e.g., a greater diversity of topics as we read more documents from Wikipedia, identify more friend groups as we process more of Facebook's network structure, etc.

<h2>Piazza Site</h2>

Our course Piazza page is here: <a href="https://piazza.com/mit/spring2020/6435">https://piazza.com/mit/spring2020/6435</a>

<h2>Description</h2>

This course will cover Bayesian modeling and inference at an advanced graduate level. A tentative list of topics (which may change depending on our interests) is as follows:
<ul>
<li> Introduction to Bayesian inference; motivations from de Finetti, decision theory, etc.
<li> Hierarchical modeling, including popular models such as latent Dirichlet allocation
<li> Approximate posterior inference
<li> Variational inference, mean-field, stochastic variational inference, challenges/limitations of VI, etc.
<li> Monte Carlo, avoiding random-walk behavior, Hamiltonian Monte Carlo/NUTS/Stan, etc.
<li> Evaluation, sensitivity, robustness
<li> Bayesian nonparametrics: why and how
<li> Mixture models, admixtures, Dirichlet process, Chinese restaurant process
<li> Feature allocations, beta process, Indian buffet process
<li> Combinatorial stochastic processes
<li> Learning functions, Gaussian processes
<li> Probabilistic numerics
<li> Bayesian optimization
</ul>

<h2>Prerequisites</h2>

<p>Requirements:  A graduate-level familiarity with machine learning/statistics and probability is required. (E.g. at MIT, 6.437 or 6.438 or [6.867 and 6.436].)

We will assume familiarity with graphical models, exponential families, finite-dimensional Gaussian mixture models, expectation maximization, linear & logistic regression, hidden Markov models.


</body>

</html>
